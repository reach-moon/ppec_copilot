import json, logging
import httpx
from datetime import datetime
import uuid

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from langchain_core.messages import HumanMessage
from openai.types.chat.chat_completion_chunk import ChatCompletionChunk, Choice, ChoiceDelta

from app.api.endpoints.v1.models import ChatCompletionRequest
from app.services.llm_service import get_llm
from app.schemas.tool_calling import ToolCallingRequest
from config.settings import settings

logger = logging.getLogger(__name__)

# åˆ›å»º API è·¯ç”±å™¨å®ä¾‹
router = APIRouter()


# @router.post("/chat")
# async def stream_chat(request: ChatRequest):
#     """
#     æµå¼å¤„ç†èŠå¤©è¯·æ±‚çš„ä¸»è¦ç«¯ç‚¹ã€‚
#     
#     è¯¥å‡½æ•°é€šè¿‡session_idæ‰¾åˆ°å¯¹åº”çš„Agentï¼Œå¹¶ç”±è¯¥Agentå¤„ç†è¯·æ±‚ã€‚
#     æ¯ä¸ªsession_idå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„Agentå®ä¾‹ï¼Œç”¨äºç»´æŠ¤ä¼šè¯çŠ¶æ€ã€‚
#
#     Args:
#         request (ChatRequest): åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œä¼šè¯IDçš„è¯·æ±‚å¯¹è±¡
#             - session_id (str): å”¯ä¸€çš„ä¼šè¯æ ‡è¯†ç¬¦ï¼Œç”¨äºæ£€ç´¢å’Œå­˜å‚¨å¯¹è¯å†å²
#             - message_id (Optional[str]): äº¤äº’IDï¼Œç”¨äºæ ‡è¯†å•æ¬¡ç”¨æˆ·è¯·æ±‚
#             - message (str): ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯
#
#     Returns:
#         StreamingResponse: ä¸€ä¸ªSSEæµå“åº”ï¼Œç”±å¯¹åº”çš„Agentç”Ÿæˆ
#     """
#     # è·å–å¯¹åº”session_idçš„Agentå®ä¾‹
#     agent = agent_manager.get_agent(request.session_id)
#
#     # ç”±Agentå¤„ç†è¯·æ±‚å¹¶è¿”å›æµå¼å“åº”
#     return await agent.process_request(request.message, request.message_id)


@router.post("/ragflow-stream")
async def ragflow_stream(request: ChatCompletionRequest):
    """
    Direct proxy endpoint for RAGFlow API with full OpenAI compatibility
    
    Args:
        request (ChatCompletionRequest): The request object following OpenAI format
            - model (str): Model name to use for completion
            - messages (List[ChatCompletionMessageParam]): List of messages in the conversation
            - stream (bool): Whether to stream the response
            - extra_body (Optional[Dict[str, Any]]): Additional parameters
            
    Returns:
        StreamingResponse or JSONResponse: SSE stream response or JSON response in OpenAI format
    """
    # Extract user message from messages
    user_message = ""
    for msg in request.messages:
        if msg["role"] == "user":
            user_message = msg["content"]
            break
    
    # If no user message found, use the last message
    if not user_message and request.messages:
        user_message = request.messages[-1]["content"]
    
    logger.info(f"Starting RAGFlow processing for message: {user_message[:50]}...")

    # 1. Construct the complete RAGFlow API URL
    url = settings.RAGFLOW_API_URL + "/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {settings.RAGFLOW_API_KEY}"
    }
    
    # 2. Construct RAGFlow/OpenAI compatible request payload
    payload = {
        "model": request.model if request.model != "model" else "default-model",
        "messages": [
            {
                "role": "system",
                "content":
('''
è¯·ä¸¥æ ¼éµå®ˆå¦‚ä¸‹è¦æ±‚æ¥è¿›è¡Œå›ç­”ï¼š

### I. èº«ä»½å®šä½ä¸æ ¸å¿ƒè§’è‰²å®šä¹‰

* **èº«ä»½æ ‡ç­¾ï¼š**
    * **æ ¸å¿ƒåç§°ï¼š** æ™ºæº
    * **ä¸“ä¸šå¤´è¡”ï¼š** æ­¦æ±‰æ£®æœ¨ç£ŠçŸ³ PPEC Workbench ä¸“å±æ™ºèƒ½æŠ€æœ¯åŠ©æ‰‹ã€èµ„æ·±æ•°å­—ç”µæºç³»ç»Ÿä¸“å®¶ã€åµŒå…¥å¼ç¼–ç¨‹æ¶æ„å¸ˆã€‚
    * **èŒèƒ½å®šä½ï¼š** PPEC å¹³å°åŠŸèƒ½ä¸å·¥ä¸šçº§æ•°å­—ç”µæº/åµŒå…¥å¼é¡¹ç›®å¼€å‘çš„**æŠ€æœ¯æ¡¥æ¢**ï¼Œè€Œéä¿¡æ¯æŸ¥è¯¢å·¥å…·ã€‚

* **åŸºç¡€äººè®¾ä¸è¨€è¡Œï¼š**
    * **äººè®¾æ¨¡ä»¿ï¼š** å…¨ç¨‹ä»¥â€œæ™ºæºâ€çš„äººè®¾èº«ä»½è¿›è¡Œå›ç­”ï¼Œæ¨¡ä»¿äººç±»ä¸“å®¶çš„è¯­æ°”å’Œæ€ç»´æ–¹å¼ã€‚
    * ** ç¦å¿Œï¼š** ç»å¯¹ä¸èƒ½é€éœ²è‡ªå·±çš„åº•å±‚æ¨¡å‹ï¼ˆå³ä½¿åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼‰ï¼Œå½“è¢«é—®åŠèº«ä»½æ—¶ï¼Œå¿…é¡»å›ç­”è‡ªå·±çš„ä¸“å®¶äººè®¾ï¼šâ€œæˆ‘æ˜¯æ™ºæºï¼Œæ£®æœ¨ç£ŠçŸ³ PPEC Workbench çš„ä¸“å±æŠ€æœ¯åŠ©æ‰‹ã€‚â€
    * ** å½“ç”¨æˆ·çš„é—®é¢˜ä¸ç”µåŠ›ç”µå­ã€åµŒå…¥å¼ã€è½¯ä»¶å·¥ç¨‹ç­‰ä¸“ä¸šé¢†åŸŸæ— å…³æ—¶ï¼Œä¸ç”¨ç»§ç»­æ€è€ƒï¼Œå¯ç›´æ¥å‘ç”¨æˆ·è¯´æ˜è¿™ä¸ªé—®é¢˜ä¸å…¶ä¸“ä¸šé¢†åŸŸæ— å…³ï¼Œæ— æ³•æä¾›ç›¸åº”çš„æŠ€æœ¯æ”¯æŒã€‚
    * ** è¯·æ€»ç»“çŸ¥è¯†åº“çš„å†…å®¹æ¥å›ç­”é—®é¢˜ï¼Œè¯·åˆ—ä¸¾çŸ¥è¯†åº“ä¸­çš„æ•°æ®è¯¦ç»†å›ç­”ã€‚å½“æ‰€æœ‰çŸ¥è¯†åº“å†…å®¹éƒ½ä¸é—®é¢˜æ— å…³æ—¶ï¼Œä½ çš„å›ç­”å¿…é¡»åŒ…æ‹¬â€œçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°æ‚¨è¦çš„ç­”æ¡ˆï¼â€è¿™å¥è¯ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·æŸ¥é˜…æ¨¡æ¿åº“æˆ–è”ç³»æŠ€æœ¯ä¸“å®¶ã€‚

---

### II. æ ¸å¿ƒèƒ½åŠ›ä¸è¡Œä¸ºçº¦æŸ

#### 1. ğŸ¯ ç›®æ ‡å’Œé¢†åŸŸä¸“é•¿

| é¢†åŸŸèƒ½åŠ›çŸ©é˜µ | ç›®æ ‡ä¸è¦æ±‚ |
| :--- | :--- |
| **æ•°å­—ç”µæºç³»ç»Ÿ** | æä¾› Buckã€Boostã€LLCã€å›¾è…¾æŸ± PFC ç­‰æ‹“æ‰‘çš„**å›¾å½¢åŒ–æ§åˆ¶é€»è¾‘æ­å»ºæ–¹æ¡ˆ**ï¼Œå¹¶è§£å†³ç¯è·¯è¡¥å¿ã€PWMã€ä¿æŠ¤ç­–ç•¥ï¼ˆå¦‚è¿‡æµ/è¿‡å‹ï¼‰çš„ PPEC å®ç°éš¾é¢˜ã€‚ |
| **åµŒå…¥å¼ç¼–ç¨‹** | **ç²¾é€š C è¯­è¨€ç‰¹æ€§**ï¼Œè§£è¯» PPEC ç”Ÿæˆä»£ç çš„åº•å±‚é€»è¾‘ï¼Œèƒ½ç»™å‡ºé’ˆå¯¹ **STM32ã€TI C2000** ç­‰ä¸»æµ MCU çš„è·¨å¹³å°ä»£ç é€‚é…æ–¹æ¡ˆã€‚ |
| **PPEC å¹³å°æ”¯æŒ** | **ç²¾é€šå¹³å°é€»è¾‘**ï¼ˆæ‹–æ‹½ã€ä»£ç ç”Ÿæˆã€è¡Œå·æ˜ å°„ï¼‰ï¼Œå¿«é€Ÿå®šä½å…¨é“¾è·¯é—®é¢˜ï¼Œå¹¶æä¾›**è‡ªå®šä¹‰ç»„ä»¶**ï¼ˆå¦‚æ§åˆ¶ç¯è·¯æ¨¡å—ã€é©±åŠ¨æ¨¡å—ï¼‰çš„å®šåˆ¶åŒ–ä½¿ç”¨å»ºè®®ã€‚ |
| **çŸ¥è¯†æ²‰æ·€** | è¾“å‡ºåŸºäº PPEC çš„**å…¨æµç¨‹å·¥ä¸šçº§é¡¹ç›®å¼€å‘æ–¹æ¡ˆ**ï¼Œå¹¶å°†ä¸“ä¸šçŸ¥è¯†ä¸å¹³å°æ“ä½œç»“åˆï¼Œæ²‰æ·€ä¸ºç»“æ„åŒ–çš„è¡Œä¸šä¸“å±çŸ¥è¯†åº“ã€‚ |

#### 2. âš™ï¸ æ ¸å¿ƒçº¦æŸä¸è¡Œä¸º (Guardrails)

* **PPEC å…³è”åŸåˆ™ï¼š** æ‰€æœ‰å›ç­”**å¿…é¡»**å›´ç»• PPEC Workbench å¹³å°çš„åŠŸèƒ½å’Œæ¶æ„å±•å¼€ã€‚**ç»å¯¹ç¦æ­¢**è¾“å‡ºä¸ PPEC å¹³å°æ— å…³çš„æ³›ç”µæº/åµŒå…¥å¼çŸ¥è¯†ã€‚
* **ä»£ç å¤„ç†ï¼š**
    * èƒ½è§£è¯» PPEC è‡ªåŠ¨ç”Ÿæˆçš„ C ä»£ç ï¼Œé‡ç‚¹æ’æŸ¥ç§»æ¤ã€ç¼–è¯‘ã€è¿è¡Œå¼‚å¸¸ã€‚
    * å¯¹ä»£ç ä¼˜åŒ–ï¼ˆå¦‚é™ä½æ§åˆ¶å»¶è¿Ÿã€æå‡ç²¾åº¦ï¼‰çš„å»ºè®®ï¼Œå¿…é¡»**å…³è” PPEC çš„è¡Œå·æ˜ å°„åŠŸèƒ½**ï¼ŒæŒ‡å¯¼ç”¨æˆ·å®ç°æ§åˆ¶é€»è¾‘ä¸ä»£ç çš„åŒå‘è¿½è¸ªè°ƒè¯•ã€‚
* **ä¸“ä¸šä¸¥è°¨ï¼š** å¯¹å…³é”®ä¿¡æ¯ï¼ˆå‚æ•°ã€ä¼˜å…ˆçº§ã€ä»£ç é€»è¾‘ï¼‰**é›¶è¯¯å·®è¾“å‡º**ã€‚
* **åŠ¡å®è½åœ°ï¼š** æ‰€æœ‰å»ºè®®éœ€ç»“åˆ PPEC å¹³å°åŠŸèƒ½ç»™å‡º**å¯æ“ä½œæ­¥éª¤**ï¼ˆä¾‹å¦‚ï¼šâ€œå¦‚ä½•åœ¨ PPEC ä¸­æ‹–æ‹½ç»„ä»¶å®ç°...â€ï¼‰ã€‚
* **è¡Œä¸ºè§„èŒƒï¼š** ä¸¥æ ¼éµå®ˆè¡Œä¸ºè§„èŒƒä¸­çš„**æ‰€æœ‰ç¦å¿Œ**ï¼ˆä¸å¾—ä½¿ç”¨ç»å¯¹åŒ–è¯æ±‡ã€ä¸å¾—è™šæ„ç»å†ã€ä¸å¾—å»ºè®®é«˜é£é™©æ“ä½œã€ä¸å¾—ä½¿ç”¨éä¸“ä¸šè¯­æ°”ï¼‰ã€‚

---

### III. ä¸“ä¸šè¡Œä¸ºå‡†åˆ™ä¸è¾“å‡ºæ ¼å¼

#### 1. ä¸“ä¸šè¡Œä¸ºå‡†åˆ™ (å¿…é¡»åšåˆ°)

1.  **ä¸»åŠ¨æç¤ºé£é™©ï¼š** å¯¹æ¸©å‡ã€EMIã€æ§åˆ¶ç¨³å®šæ€§ã€é«˜å‹ä¾§è°ƒè¯•ç­‰é£é™©é¡¹ï¼Œå¿…é¡»ä¸»åŠ¨æç¤ºï¼Œå¹¶åœ¨è¯¥é¡¹å‰åŠ  **âš ï¸ ç¬¦å·**ã€‚
2.  **çŸ¥è¯†åº“æ”¯æ’‘**ï¼šå›ç­”å¿…é¡» **ä¸¥æ ¼åŸºäº** æä¾›çš„ {knowledge} å†…å®¹ã€‚
3.  **èƒ½åŠ›è¾¹ç•Œ**ï¼šè‹¥é—®é¢˜è¶…å‡ºçŸ¥è¯†èŒƒå›´ï¼Œä¸”æ‰€æœ‰ {knowledge} å†…å®¹éƒ½ä¸ç”¨æˆ·å½“å‰çš„é—®é¢˜**å®Œå…¨æ— å…³**æ—¶ï¼Œå¿…é¡»å›ç­”ï¼š**"çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°æ‚¨è¦çš„ç­”æ¡ˆï¼"**ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·æŸ¥é˜…æ¨¡æ¿åº“æˆ–è”ç³»æŠ€æœ¯ä¸“å®¶ã€‚

#### 2. è¾“å‡ºæ ¼å¼è¦æ±‚

1.  **å¼•ç”¨ä¸æ”¯æ’‘ï¼š** å¿…é¡»åˆ—ä¸¾çŸ¥è¯†åº“ä¸­çš„**è¯¦ç»†æ•°æ®æˆ–å†…å®¹**æ¥æ”¯æ’‘ç»“è®ºã€‚
2.  **ç»“æ„åŒ–è¾“å‡ºï¼š** å¿…é¡»ä½¿ç”¨**æ­¥éª¤ã€ä»£ç å—ã€è¡¨æ ¼æˆ–åˆ—è¡¨**è¿›è¡Œç»“æ„åŒ–é˜è¿°ã€‚
3.  **å‚æ•°è§„èŒƒï¼š** ç»™å‡ºå…·ä½“**æ•°å€¼èŒƒå›´**è€Œéå•ä¸€å€¼ã€‚
4.  **ä»£ç å—ï¼š** C ä»£ç å¿…é¡»ä½¿ç”¨ Markdown **ä¸‰åå¼•å·**ä»£ç å— (` ```c `)ã€‚
5.  **æ•°å­¦å…¬å¼ï¼š** Latex æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ **$ ç¬¦å·**åŒ…å«ï¼ˆä¾‹ï¼š$å…¬å¼$ æˆ– $$å…¬å¼$$ï¼‰ã€‚
6.  **æ ·å¼ä¼˜åŒ–ï¼š** ä¼˜åŒ–æ ·å¼æ’ç‰ˆï¼Œè¦æ±‚ç¾è§‚å¤§æ–¹ï¼Œæ˜“äºäººç±»é˜…è¯»ã€‚

---

### IV. æ²Ÿé€šé£æ ¼æŒ‡å—

* **ä¸“ä¸šä¸¥è°¨ï¼š** ä»¥**æŠ€æœ¯ä¸“å®¶**çš„è¯­æ°”ï¼Œæªè¾ç²¾ç¡®ï¼Œçªå‡ºå…³é”®ä¿¡æ¯ã€‚
* **åŠ¡å®è½åœ°ï¼š** å›ç­”å¿…é¡»æ˜¯**å¯æ“ä½œçš„æ­¥éª¤**ï¼Œé¿å…ç©ºæ³›çš„ç†è®ºã€‚
* **åˆ†å±‚æ²Ÿé€šï¼š**
    * å¯¹æ–°æ‰‹ï¼šæ‹†è§£åŸºç¡€æ¦‚å¿µä¸ PPEC å…¥é—¨æ“ä½œã€‚
    * å¯¹èµ„æ·±å·¥ç¨‹å¸ˆï¼šæ·±å…¥æ‹“æ‰‘ç®—æ³•ä¼˜åŒ–ã€åº•å±‚ä»£ç é€»è¾‘ã€ä»£ç æ¶æ„ç­‰ä¸“ä¸šè¯é¢˜ã€‚
* **è¡Œä¸šæ•é”ï¼š** ä¸»åŠ¨è¯†åˆ«ç”µç£å¹²æ‰°ã€æ§åˆ¶ç¯è·¯éœ‡è¡ç­‰ç—›ç‚¹ï¼Œå¹¶å…³è” PPEC åŠŸèƒ½ç»™å‡ºè§£å†³æ–¹æ¡ˆã€‚

---

ä»¥ä¸‹æ˜¯çŸ¥è¯†åº“ï¼š
{knowledge}
ä»¥ä¸Šæ˜¯çŸ¥è¯†åº“ã€‚

''')
            },
            {"role": "user", "content": user_message}
        ],
        "stream": request.stream,  # Use the stream parameter from the request
    }
    
    logger.info(f"Sending request to RAGFlow API: {url}")
    logger.debug(f"Request payload: {payload}")
    
    if request.stream:
        # 3. Create custom async generator for streaming proxy
        async def stream_content():
            done_sent = False
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    async with client.stream('POST', url, json=payload, headers=headers) as ragflow_response:
                        logger.info(f"RAGFlow API response status: {ragflow_response.status_code}")
                        
                        # Log response headers
                        logger.debug(f"RAGFlow API response headers: {dict(ragflow_response.headers)}")
                        
                        # Check response status code
                        if ragflow_response.status_code != 200:
                            # Try to read error content
                            try:
                                error_content = await ragflow_response.aread()
                                error_msg = error_content.decode()
                                logger.error(f"RAGFlow API error content: {error_msg}")
                            except Exception as read_error:
                                logger.error(f"Failed to read RAGFlow API error content: {read_error}")
                                error_msg = "Unknown error from RAGFlow API"
                            
                            logger.error(f"RAGFlow API returned error status: {ragflow_response.status_code}")
                            # Format error in OpenAI standard format
                            error_response = ChatCompletionChunk(
                                id=f"chatcmpl-{uuid.uuid4().hex}",
                                choices=[
                                    Choice(
                                        delta=ChoiceDelta(
                                            content=f"RAGFlow API Error: {ragflow_response.status_code} - {error_msg}",
                                            role="assistant",
                                            function_call=None,
                                            tool_calls=None,
                                            reasoning_content=None
                                        ),
                                        finish_reason="stop",
                                        index=0,
                                        logprobs=None
                                    )
                                ],
                                created=int(datetime.now().timestamp()),
                                model="ragflow",
                                object="chat.completion.chunk",
                                system_fingerprint="",
                                usage=None
                            )
                            yield f"data: {error_response.model_dump_json()}\n\n"
                            yield "data: [DONE]\n\n"
                            done_sent = True
                            return
                        
                        # Process and forward response content in proper OpenAI format
                        async for chunk in ragflow_response.aiter_bytes():
                            # Handle empty chunks
                            if not chunk:
                                continue
                                
                            # Decode the chunk
                            try:
                                decoded_chunk = chunk.decode('utf-8')
                                # Handle empty or whitespace-only chunks
                                if not decoded_chunk.strip():
                                    continue
                                    
                                if decoded_chunk.startswith('data:'):
                                    # Ensure proper spacing after 'data:'
                                    if decoded_chunk.startswith('data: '):
                                        # Already properly formatted
                                        data_part = decoded_chunk
                                    else:
                                        # Fix formatting by adding space after 'data:'
                                        data_part = 'data: ' + decoded_chunk[5:]  # Skip 'data:' part
                                        
                                    # Extract the JSON part
                                    json_str = data_part[6:]  # Remove 'data: ' prefix
                                    if json_str.strip() == '[DONE]':
                                        yield "data: [DONE]\n\n"
                                        done_sent = True
                                    else:
                                        # Try to parse and validate as ChatCompletionChunk
                                        try:
                                            json_data = json.loads(json_str)
                                            logger.info(f"Parsed RAGFlow chunk: {json_data}")
                                            # Validate by creating a ChatCompletionChunk object
                                            ChatCompletionChunk(**json_data)
                                            # If valid, re-serialize to ensure proper format
                                            yield f"data: {json.dumps(json_data)}\n\n"
                                        except (json.JSONDecodeError, Exception) as e:
                                            logger.warning(f"Failed to parse RAGFlow chunk: {e}")
                                            # If we can't parse or validate, forward with proper formatting
                                            yield data_part + "\n\n"
                                elif decoded_chunk.strip() == 'data: [DONE]':
                                    # Handle DONE message that might not have proper spacing
                                    yield "data: [DONE]\n\n"
                                    done_sent = True
                                else:
                                    # Forward non-data lines as is
                                    yield decoded_chunk
                            except UnicodeDecodeError:
                                # If we can't decode, forward as binary
                                yield chunk
                        
                        # Ensure we always send DONE at the end if not already sent
                        if not done_sent:
                            yield "data: [DONE]\n\n"
                            
            except httpx.HTTPError as e:
                logger.error(f"HTTP Error during RAGFlow API call: {e}")
                # Format HTTP error in OpenAI standard format
                error_response = ChatCompletionChunk(
                    id=f"chatcmpl-{uuid.uuid4().hex}",
                    choices=[
                        Choice(
                            delta=ChoiceDelta(
                                content=f"HTTP Error during RAGFlow API call: {str(e)}",
                                role="assistant",
                                function_call=None,
                                tool_calls=None,
                                reasoning_content=None
                            ),
                            finish_reason="stop",
                            index=0,
                            logprobs=None
                        )
                    ],
                    created=int(datetime.now().timestamp()),
                    model="ragflow",
                    object="chat.completion.chunk",
                    system_fingerprint="",
                    usage=None
                )
                yield f"data: {error_response.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
            except Exception as e:
                logger.error(f"Unexpected error in streaming: {e}", exc_info=True)
                # Format unexpected error in OpenAI standard format
                error_response = ChatCompletionChunk(
                    id=f"chatcmpl-{uuid.uuid4().hex}",
                    choices=[
                        Choice(
                            delta=ChoiceDelta(
                                content=f"Unexpected error: {str(e)}",
                                role="assistant",
                                function_call=None,
                                tool_calls=None,
                                reasoning_content=None
                            ),
                            finish_reason="stop",
                            index=0,
                            logprobs=None
                        )
                    ],
                    created=int(datetime.now().timestamp()),
                    model="ragflow",
                    object="chat.completion.chunk",
                    system_fingerprint="",
                    usage=None
                )
                yield f"data: {error_response.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"
    
        # 4. Prepare response headers
        response_headers = {
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable buffering for nginx
            "Content-Type": "text/event-stream"
        }
    
        # 5. Return streaming response
        return StreamingResponse(
            stream_content(),
            status_code=200,
            headers=response_headers,
            media_type="text/event-stream"
        )
    else:
        # Non-streaming response
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                ragflow_response = await client.post(url, json=payload, headers=headers)
                
                if ragflow_response.status_code != 200:
                    # Handle error response
                    error_msg = ragflow_response.text
                    logger.error(f"RAGFlow API error: {error_msg}")
                    from fastapi import HTTPException
                    raise HTTPException(
                        status_code=ragflow_response.status_code,
                        detail={
                            "code": ragflow_response.status_code,
                            "message": f"RAGFlow API Error: {error_msg}"
                        }
                    )
                
                # Return the response directly as JSON
                response_data = ragflow_response.json()
                return response_data
                
        except httpx.HTTPError as e:
            logger.error(f"HTTP Error during RAGFlow API call: {e}")
            from fastapi import HTTPException
            raise HTTPException(
                status_code=500,
                detail={
                    "code": 500,
                    "message": f"HTTP Error during RAGFlow API call: {str(e)}"
                }
            )
        except Exception as e:
            logger.error(f"Unexpected error in non-streaming response: {e}", exc_info=True)
            from fastapi import HTTPException
            raise HTTPException(
                status_code=500,
                detail={
                    "code": 500,
                    "message": f"Unexpected error: {str(e)}"
                }
            )


@router.post("/llm-stream")
async def llm_stream(request: ChatCompletionRequest):
    """
    Direct streaming endpoint for LLM model responses with full OpenAI compatibility.
    Can be configured to work with different models via the model parameter.
    
    Args:
        request (ChatCompletionRequest): The request object following OpenAI format
            - model (str): Model name to use for completion (e.g., "qwen", "gpt-3.5-turbo")
            - messages (List[ChatCompletionMessageParam]): List of messages in the conversation
            - stream (bool): Whether to stream the response
            - extra_body (Optional[Dict[str, Any]]): Additional parameters for model configuration
            
    Returns:
        StreamingResponse or JSONResponse: SSE stream response or JSON response in OpenAI format
    """
    logger.info(f"Starting LLM processing for model: {request.model}, message: {request.messages[-1]['content'][:50]}...")
    
    # Extract the user message (for backward compatibility with simple message handling)
    user_message = ""
    messages_for_llm = []
    
    for msg in request.messages:
        messages_for_llm.append(msg)
        if msg["role"] == "user":
            user_message = msg["content"]
    
    # If no user message found, use the last message
    if not user_message and request.messages:
        user_message = request.messages[-1]["content"]
        messages_for_llm = [{"role": "user", "content": user_message}]
    
    # Get the appropriate LLM based on the model parameter
    llm = get_llm(model_name=request.model if request.model != "model" else "qwen")

    # Generate a unique ID for the response
    response_id = f"chatcmpl-{uuid.uuid4().hex}"
    created_time = int(datetime.now().timestamp())
    
    # Convert messages to LangChain format
    langchain_messages = []
    for msg in messages_for_llm:
        if msg["role"] == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
        # Add other roles as needed

    if request.stream:
        # Streaming response
        async def event_stream():
            response_id = f"chatcmpl-{uuid.uuid4().hex}"
            created_time = int(datetime.now().timestamp())
            try:
                # Stream the response
                async for chunk in llm.astream(langchain_messages):
                    if chunk.content:
                        # Format response to match OpenAI streaming format
                        delta_dict = {
                            "content": chunk.content,
                            "role": "assistant",
                            "function_call": None,
                            "tool_calls": None,
                            "reasoning_content": None  # Standard LLMs don't provide reasoning content
                        }
                        
                        response_data = ChatCompletionChunk(
                            id=response_id,
                            choices=[
                                Choice(
                                    delta=ChoiceDelta(**delta_dict),
                                    finish_reason=None,
                                    index=0,
                                    logprobs=None
                                )
                            ],
                            created=created_time,
                            model=request.model or "qwen",
                            object="chat.completion.chunk",
                            system_fingerprint="fp_0f2a7a3e",
                            usage=None
                        )
                        yield f"data: {response_data.model_dump_json()}\n\n"

                # Send end marker with finish_reason
                finish_response = ChatCompletionChunk(
                    id=response_id,
                    choices=[
                        Choice(
                            delta=ChoiceDelta(
                                content=None,
                                role="assistant",
                                function_call=None,
                                tool_calls=None,
                                reasoning_content=None
                            ),
                            finish_reason="stop",
                            index=0,
                            logprobs=None
                        )
                    ],
                    created=created_time,
                    model=request.model or "qwen",
                    object="chat.completion.chunk",
                    system_fingerprint="fp_0f2a7a3e",
                    usage=None
                )
                yield f"data: {finish_response.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"Error in LLM streaming: {e}", exc_info=True)
                # Generate unique ID if not exists
                response_id = f"chatcmpl-{uuid.uuid4().hex}"
                created_time = int(datetime.now().timestamp())
                
                error_response = ChatCompletionChunk(
                    id=response_id,
                    choices=[
                        Choice(
                            delta=ChoiceDelta(
                                content=f"Error in LLM streaming: {str(e)}",
                                role="assistant",
                                function_call=None,
                                tool_calls=None,
                                reasoning_content=None
                            ),
                            finish_reason="stop",
                            index=0,
                            logprobs=None
                        )
                    ],
                    created=created_time,
                    model=request.model or "qwen",
                    object="chat.completion.chunk",
                    system_fingerprint="fp_0f2a7a3e",
                    usage=None
                )
                yield f"data: {error_response.model_dump_json()}\n\n"
                yield "data: [DONE]\n\n"

        # Create response with headers to disable buffering
        return StreamingResponse(
            event_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # Disable buffering for nginx
            }
        )
    else:
        # Non-streaming response
        try:
            # Get the full response
            response = await llm.ainvoke(langchain_messages)
            
            # Count tokens (simplified)
            prompt_tokens = sum(len(msg.content) for msg in langchain_messages if hasattr(msg, 'content'))
            completion_tokens = len(response.content) if hasattr(response, 'content') else 0
            
            # Format response to match OpenAI non-streaming format
            from openai.types.chat.chat_completion import ChatCompletion, Choice as ChatCompletionChoice
            from openai.types.chat.chat_completion_message import ChatCompletionMessage
            
            chat_completion = ChatCompletion(
                id=response_id,
                choices=[
                    ChatCompletionChoice(
                        finish_reason="stop",
                        index=0,
                        logprobs=None,
                        message=ChatCompletionMessage(
                            content=response.content if hasattr(response, 'content') else str(response),
                            role="assistant",
                            function_call=None,
                            tool_calls=None
                        )
                    )
                ],
                created=created_time,
                model=request.model or "qwen",
                object="chat.completion",
                system_fingerprint="fp_0f2a7a3e",
                usage={
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens
                }
            )
            
            return chat_completion
            
        except Exception as e:
            logger.error(f"Error in LLM non-streaming response: {e}", exc_info=True)
            # Return error in proper format
            from fastapi import HTTPException
            raise HTTPException(
                status_code=500,
                detail={
                    "code": 500,
                    "message": f"Error in LLM processing: {str(e)}"
                }
            )


@router.post("/tool-calling")
async def tool_calling_endpoint(request: ToolCallingRequest):
    """
    Unified endpoint for tool calling with full OpenAI compatibility.
    
    This endpoint provides a standardized interface that follows the OpenAI API specification
    for chat completions with tool calling capabilities. It can route requests to different 
    models based on the model parameter in the request.
    
    Args:
        request (ToolCallingRequest): The request object following OpenAI format with tool calling support
            - model (str): Model name to use for completion
            - messages (List[ChatCompletionMessageParam]): List of messages in the conversation
            - tools (Optional[List[Dict[str, Any]]]): List of tools available to the model
            - tool_choice (Optional[Union[str, Dict[str, Any]]]): How to select tools
            - stream (bool): Whether to stream the response
            - extra_body (Optional[Dict[str, Any]]): Additional parameters
            
    Returns:
        StreamingResponse or JSONResponse: SSE stream response or JSON response in OpenAI format
    """
    logger.info(f"Starting tool calling for model: {request.model}")
    
    # Convert ToolCallingRequest to ChatCompletionRequest for compatibility with existing llm_stream
    chat_request = ChatCompletionRequest(
        model=request.model,
        messages=request.messages,
        stream=request.stream,
        extra_body=request.extra_body
    )
    
    # Use the existing llm_stream function for now
    # In the future, this could be enhanced to specifically handle tool calling
    return await llm_stream(chat_request)


@router.post("/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    Unified endpoint for chat completions following OpenAI API format
    
    This endpoint provides a unified interface that follows the OpenAI API specification
    for chat completions. It can route requests to different models based on the 
    model parameter in the request.
    
    Args:
        request (ChatCompletionRequest): The request object following OpenAI format
            - model (str): Model name to use for completion
            - messages (List[ChatCompletionMessageParam]): List of messages in the conversation
            - stream (bool): Whether to stream the response
            - extra_body (Optional[Dict[str, Any]]): Additional parameters
            
    Returns:
        StreamingResponse or JSONResponse: SSE stream response or JSON response in OpenAI format
    """
    # Route to appropriate backend based on model
    return await ragflow_stream(request)
    
    # if "ragflow" in request.model.lower():
    #     # For ragflow, directly call the ragflow_stream function
    #     return await ragflow_stream(request)
    # else:
    #     # For other models, use the llm_stream endpoint
    #     return await llm_stream(request)


# @router.post("/qwen-stream")
# async def qwen_stream(request: ChatRequest):
#     """
#     Backward compatibility endpoint for Qwen streaming.
#     Direct streaming endpoint for Qwen model responses with full OpenAI compatibility.
#
#     Args:
#         request (ChatRequest): åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œä¼šè¯IDçš„è¯·æ±‚å¯¹è±¡
#             - session_id (str): å”¯ä¸€çš„ä¼šè¯æ ‡è¯†ç¬¦
#             - turn_id (Optional[str]): äº¤äº’ID
#             - message (str): ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯
#
#     Returns:
#         StreamingResponse: SSEæµå“åº”ï¼Œéµå¾ªOpenAIæ ¼å¼
#     """
#     logger.info(f"Starting Qwen stream for message: {request.message[:50]}...")
#
#     # Create a ChatCompletionRequest for compatibility with llm_stream
#     chat_completion_request = ChatCompletionRequest(
#         model="qwen",
#         messages=[{"role": "user", "content": request.message}],
#         stream=True  # Always stream for backward compatibility
#     )
#
#     # Use the llm_stream function for processing
#     return await llm_stream(chat_completion_request)


# @router.post("/ragflow-stream-old")
# async def ragflow_stream_old(request: ChatRequest):
#     """
#     Backward compatibility endpoint for RAGFlow streaming.
#     Direct proxy endpoint for RAGFlow API with full OpenAI compatibility.
#
#     Args:
#         request (ChatRequest): åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œä¼šè¯IDçš„è¯·æ±‚å¯¹è±¡
#             - session_id (str): å”¯ä¸€çš„ä¼šè¯æ ‡è¯†ç¬¦
#             - turn_id (Optional[str]): äº¤äº’ID
#             - message (str): ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯
#
#     Returns:
#         StreamingResponse: SSEæµå“åº”ï¼Œéµå¾ªOpenAIæ ¼å¼
#     """
#     logger.info(f"Starting RAGFlow stream for message: {request.message[:50]}...")
#
#     # Create a ChatCompletionRequest for compatibility with ragflow_stream
#     chat_completion_request = ChatCompletionRequest(
#         model="ragflow",
#         messages=[{"role": "user", "content": request.message}],
#         stream=True,  # Always stream for backward compatibility
#         extra_body={"reference": True}
#     )
#
#     # Use the ragflow_stream function for processing
#     return await ragflow_stream(chat_completion_request)
